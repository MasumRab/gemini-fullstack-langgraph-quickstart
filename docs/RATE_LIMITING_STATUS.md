# Rate Limiting Implementation Status

## Date: 2025-12-12

## Overview
This document confirms that rate limiting is **properly implemented** across all agent nodes to prevent hitting Gemini API rate limits (RPM, TPM, and RPD).

## Rate Limiting Architecture

### Core Components

1. **`RateLimiter` class** (`backend/src/agent/rate_limiter.py`)
   - Thread-safe rate limiting for Gemini API calls
   - Tracks three dimensions:
     - **RPM** (Requests Per Minute)
     - **TPM** (Tokens Per Minute)  
     - **RPD** (Requests Per Day)

2. **`ContextWindowManager` class** (`backend/src/agent/rate_limiter.py`)
   - Manages context window sizes
   - Truncates prompts to fit within model limits
   - Estimates token counts

3. **`_get_rate_limited_llm()` helper** (`backend/src/agent/nodes.py`)
   - Centralized function for creating rate-limited LLM instances
   - Automatically applies rate limiting before each API call
   - Logs usage statistics

## Gemini 2.5 Rate Limits (Free Tier)

### gemini-2.5-flash & gemini-2.5-flash-lite
- **RPM**: 15 requests/minute
- **TPM**: 1,000,000 tokens/minute
- **RPD**: 1,500 requests/day
- **Max Context**: 1,048,576 tokens
- **Max Output**: 8,192 tokens

### gemini-2.5-pro
- **RPM**: 10 requests/minute (lower)
- **TPM**: 1,000,000 tokens/minute
- **RPD**: 1,000 requests/day (lower)
- **Max Context**: 2,097,152 tokens
- **Max Output**: 8,192 tokens

## Rate Limiting Implementation by Node

### ‚úÖ Nodes with Rate Limiting

| Node | Line | Rate Limited | Model Used |
|------|------|--------------|------------|
| `scoping_node` | 123-127 | ‚úÖ Yes | `gemini-2.5-flash` |
| `generate_query` | 204-209 | ‚úÖ Yes | `configurable.query_generator_model` |
| `validate_web_results` | 493-497 | ‚úÖ Yes | `app_config.model_validation` |
| `compression_node` | 572-576 | ‚úÖ Yes | `app_config.model_compression` |
| `reflection` | 615-620 | ‚úÖ Yes | `configurable.reflection_model` |
| `finalize_answer` | 687-692 | ‚úÖ Yes | `configurable.answer_model` |

### ‚ö†Ô∏è Nodes without LLM calls (No rate limiting needed)

- `load_context` - Persistence only
- `continue_to_web_research` - Routing only
- `web_research` - Uses Google Search (different API)
- `planning_mode` - State management only
- `planning_wait` - Interrupt only
- `evaluate_research` - Routing only

## How Rate Limiting Works

### 1. Before Each API Call

```python
llm = _get_rate_limited_llm(
    model="gemini-2.5-flash",
    temperature=0,
    prompt=formatted_prompt  # Used for token estimation
)
```

### 2. Inside `_get_rate_limited_llm()`

```python
# Get rate limiter for this model
rate_limiter = get_rate_limiter(model)
context_mgr = get_context_manager(model)

# Estimate tokens from prompt
estimated_tokens = context_mgr.estimate_tokens(prompt)

# Wait if necessary to stay within limits
wait_time = rate_limiter.wait_if_needed(estimated_tokens)

# Log usage statistics
usage = rate_limiter.get_current_usage()
logger.debug(f"RPM={usage['rpm']}/{usage['rpm_limit']}, TPM={usage['tpm']}/{usage['tpm_limit']}, RPD={usage['rpd']}/{usage['rpd_limit']}")
```

### 3. Rate Limiter Logic

The `wait_if_needed()` method:

1. **Cleans up old requests** (outside 1-minute and 1-day windows)
2. **Checks RPM limit**: If at limit, waits until oldest request expires
3. **Checks TPM limit**: If tokens would exceed limit, waits for token window to clear
4. **Checks RPD limit**: If daily quota reached, raises exception
5. **Records the request** with timestamp and token count

## Protection Mechanisms

### 1. Automatic Waiting
- If RPM limit reached: Waits up to 60 seconds for window to clear
- If TPM limit reached: Waits for oldest tokens to expire from window
- Logs all wait times for debugging

### 2. Daily Quota Protection
- Tracks requests per day
- Raises exception if daily limit exceeded
- Prevents unexpected API billing

### 3. Context Window Management
- Truncates prompts that exceed model limits
- Estimates tokens before API calls
- Prevents context overflow errors

### 4. Thread Safety
- Uses locks to prevent race conditions
- Safe for parallel node execution
- Global rate limiter instances per model

## Verification

### Test Rate Limiting

```python
from agent.rate_limiter import get_rate_limiter

# Get limiter for gemini-2.5-flash
limiter = get_rate_limiter("gemini-2.5-flash")

# Check current usage
usage = limiter.get_current_usage()
print(f"RPM: {usage['rpm']}/{usage['rpm_limit']}")
print(f"TPM: {usage['tpm']}/{usage['tpm_limit']}")
print(f"RPD: {usage['rpd']}/{usage['rpd_limit']}")
```

### Monitor Logs

Rate limiting events are logged:
- `INFO`: Rate limiter initialization
- `WARNING`: When waiting due to rate limits
- `DEBUG`: Usage statistics after each call
- `ERROR`: When daily quota exceeded

## Best Practices

### ‚úÖ Current Implementation

1. **Centralized rate limiting** via `_get_rate_limited_llm()`
2. **Per-model rate limiters** (different limits for Flash vs Pro)
3. **Token estimation** before API calls
4. **Automatic waiting** instead of failing
5. **Usage logging** for monitoring

### üîÑ Potential Improvements

1. **More accurate token counting** (currently uses 4 chars/token approximation)
2. **Configurable rate limits** via environment variables
3. **Rate limit metrics** exported to monitoring system
4. **Backoff strategies** for repeated rate limit hits
5. **Pacific time midnight reset** for RPD (currently 24-hour rolling window)

## Conclusion

‚úÖ **Rate limiting is fully implemented** across all LLM-calling nodes in the agent.

The implementation:
- Prevents RPM, TPM, and RPD limit violations
- Uses Gemini 2.5 model-specific limits
- Automatically waits when approaching limits
- Provides comprehensive logging
- Is thread-safe for parallel execution

**No additional rate limiting changes are needed** - the system is already protecting against API rate limit errors.
