{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Research Agent: Core Workflow\n",
    "\n",
    "This notebook demonstrates the core research workflow of the Deep Research Agent, including:\n",
    "1. Query Decomposition\n",
    "2. Web Search & RAG Ingestion\n",
    "3. Evidence Auditing & Verification\n",
    "4. Final Synthesis\n",
    "5. Artifact Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "def setup_environment():\n",
    "    \"\"\"Setup environment for Google Colab or local execution.\"\"\"\n",
    "    try:\n",
    "        import google.colab\n",
    "        IN_COLAB = True\n",
    "    except ImportError:\n",
    "        IN_COLAB = False\n",
    "\n",
    "    if IN_COLAB:\n",
    "        print(\"Detected Google Colab environment. Installing dependencies...\")\n",
    "        \n",
    "        # 1. Uninstall conflicting packages (protobuf versions)\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", \"google-ai-generativelanguage\", \"tensorflow\", \"grpcio-status\"])\n",
    "        \n",
    "        # 2. Clone repository if needed\n",
    "        if not os.path.exists(\"gemini-fullstack-langgraph-quickstart\"):\n",
    "            subprocess.check_call([\"git\", \"clone\", \"https://github.com/GoogleCloudPlatform/gemini-fullstack-langgraph-quickstart.git\"])\n",
    "            os.chdir(\"gemini-fullstack-langgraph-quickstart\")\n",
    "        elif os.path.exists(\"backend\"): \n",
    "            # Already in root\n",
    "            pass\n",
    "        \n",
    "        # 3. Install backend with strict dependency resolution\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-e\", \"backend\"])\n",
    "        \n",
    "        print(\"Dependencies installed.\")\n",
    "    else:\n",
    "        print(\"Running locally. Assuming dependencies are installed.\")\n",
    "\n",
    "setup_environment()\n",
    "\n",
    "# Add backend/src to path regardless of install method to ensure modules are found\n",
    "# This fixes the ModuleNotFoundError even if pip install -e . is used but kernel not restarted or paths not propagated\n",
    "if os.path.exists(\"backend/src\"):\n",
    "    sys.path.append(os.path.abspath(\"backend/src\"))\n",
    "elif os.path.exists(\"../backend/src\"):\n",
    "    sys.path.append(os.path.abspath(\"../backend/src\"))\n",
    "elif os.path.exists(\"gemini-fullstack-langgraph-quickstart/backend/src\"):\n",
    "    sys.path.append(os.path.abspath(\"gemini-fullstack-langgraph-quickstart/backend/src\"))\n",
    "\n",
    "print(f\"Current sys.path: {sys.path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add backend/src to path so we can import modules\n",
    "sys.path.append(os.path.abspath(\"../backend/src\"))\n",
    "\n",
    "# Set API Keys (User should replace these or set in env)\n",
    "# os.environ[\"GOOGLE_API_KEY\"] = \"...\"\n",
    "# os.environ[\"TAVILY_API_KEY\"] = \"...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent.deep_search_agent import DeepSearchAgent\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Agent Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM\n",
    "# Using Gemini as per repo config, assuming env vars are set\n",
    "try:\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", temperature=0)\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing LLM: {e}\")\n",
    "    # Fallback for testing without API keys (Mock)\n",
    "    class MockLLM:\n",
    "        def invoke(self, prompt): return \"Mock response\"\n",
    "        def generate(self, prompt): return \"Mock response\"\n",
    "    llm = MockLLM()\n",
    "\n",
    "# Initialize Agent\n",
    "agent = DeepSearchAgent(llm_client=llm)\n",
    "print(\"Agent initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Single Query Research Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Latest advances in renewable energy storage\"\n",
    "print(f\"Starting research on: {query}\")\n",
    "\n",
    "final_answer = agent.research(query)\n",
    "\n",
    "print(\"\\n=== FINAL ANSWER ===\\n\")\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. RAG Verification Deep Dive\n",
    "\n",
    "Inspect the internal state of the RAG system to verify ingestion and retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect Doc Store\n",
    "print(f\"Total documents indexed: {len(agent.rag.doc_store)}\")\n",
    "\n",
    "# Sample a document\n",
    "if len(agent.rag.doc_store) > 0:\n",
    "    doc_id = list(agent.rag.doc_store.keys())[0]\n",
    "    print(f\"\\nSample Document ({doc_id}):\")\n",
    "    print(agent.rag.doc_store[doc_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Retrieval\n",
    "test_query = \"battery cost reduction\"\n",
    "results = agent.rag.retrieve(test_query, top_k=3)\n",
    "\n",
    "print(f\"\\nRetrieval results for '{test_query}':\")\n",
    "for doc, score in results:\n",
    "    print(f\"- [{score:.2f}] {doc.content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Export Research Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = agent.rag.export_state()\n",
    "print(\"RAG State Export:\", state)\n",
    "\n",
    "# Retrieve all docs for export\n",
    "all_docs = agent.get_retrieved_documents()\n",
    "print(f\"\\nPrepared {len(all_docs)} documents for export.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}