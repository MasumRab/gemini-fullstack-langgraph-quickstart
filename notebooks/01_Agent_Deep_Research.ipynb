{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- COLAB SETUP START ---\n",
                "try:\n",
                "    import google.colab\n",
                "    IN_COLAB = True\n",
                "except ImportError:\n",
                "    IN_COLAB = False\n",
                "\n",
                "if IN_COLAB:\n",
                "    # 1. Clone the repository\n",
                "    !git clone https://github.com/google-gemini/gemini-fullstack-langgraph-quickstart\n",
                "    %cd gemini-fullstack-langgraph-quickstart/backend\n",
                "\n",
                "    # 2. Prepare Environment (Resolving Conflicts)\n",
                "    import sys\n",
                "    print(\"Uninstalling conflicting pre-installed packages...\")\n",
                "    !pip uninstall -y google-ai-generativelanguage tensorflow grpcio-status\n",
                "\n",
                "    # Pre-install PyTorch Nightly if Python 3.13+ is detected\n",
                "    if sys.version_info >= (3, 13):\n",
                "        print(\"Detected Python 3.13+. Installing PyTorch Nightly...\")\n",
                "        !pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cpu\n",
                "\n",
                "    # 3. Install Backend\n",
                "    !pip install .\n",
                "\n",
                "    # 4. Set API Key\n",
                "    import os\n",
                "    from google.colab import userdata\n",
                "    try:\n",
                "        os.environ[\"GEMINI_API_KEY\"] = userdata.get('GEMINI_API_KEY')\n",
                "    except:\n",
                "        print(\"Please enter your Gemini API Key:\")\n",
                "        os.environ[\"GEMINI_API_KEY\"] = input()\n",
                "# --- COLAB SETUP END ---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- MODEL CONFIGURATION ---\n",
                "# @title Select Gemini Model\n",
                "# @markdown Choose the model strategy to use.\n",
                "# @markdown *Note: Experimental models (2.0/2.5) may require billing/quota enablement.*\n",
                "\n",
                "MODEL_STRATEGY = \"Free Tier (Gemini 1.5 Flash)\" # @param [\"Free Tier (Gemini 1.5 Flash)\", \"Experimental (Gemini 2.0 Flash)\", \"Flash Lite (Gemini 2.0 Flash-Lite)\", \"Gemini 2.5 Flash (Experimental)\", \"Gemini 2.5 Pro (Experimental)\"]\n",
                "\n",
                "import os\n",
                "\n",
                "# Map selection to actual model names\n",
                "if MODEL_STRATEGY == \"Free Tier (Gemini 1.5 Flash)\":\n",
                "    SELECTED_MODEL = \"gemini-1.5-flash\"\n",
                "    print(\"âœ… Using Free Tier Model: gemini-1.5-flash\")\n",
                "elif MODEL_STRATEGY == \"Experimental (Gemini 2.0 Flash)\":\n",
                "    SELECTED_MODEL = \"gemini-2.0-flash-exp\"\n",
                "    print(\"âš ï¸ Using Experimental Model: gemini-2.0-flash-exp (Quota Required)\")\n",
                "elif MODEL_STRATEGY == \"Flash Lite (Gemini 2.0 Flash-Lite)\":\n",
                "    SELECTED_MODEL = \"gemini-2.0-flash-lite-preview-02-05\"\n",
                "    print(\"âš ï¸ Using Preview Model: gemini-2.0-flash-lite-preview-02-05 (Quota Required)\")\n",
                "elif MODEL_STRATEGY == \"Gemini 2.5 Flash (Experimental)\":\n",
                "    SELECTED_MODEL = \"gemini-2.5-flash\"\n",
                "    print(\"ðŸ”¥ Using Gemini 2.5 Flash: gemini-2.5-flash (Quota Required)\")\n",
                "elif MODEL_STRATEGY == \"Gemini 2.5 Pro (Experimental)\":\n",
                "    SELECTED_MODEL = \"gemini-2.5-pro\"\n",
                "    print(\"ðŸ”¥ Using Gemini 2.5 Pro: gemini-2.5-pro (Quota Required)\")\n",
                "\n",
                "# Set Environment Variables to override defaults\n",
                "os.environ[\"QUERY_GENERATOR_MODEL\"] = SELECTED_MODEL\n",
                "os.environ[\"REFLECTION_MODEL\"] = SELECTED_MODEL\n",
                "os.environ[\"ANSWER_MODEL\"] = SELECTED_MODEL\n",
                "os.environ[\"TOOLS_MODEL\"] = SELECTED_MODEL"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Deep Research Agent: Core Workflow\n",
                "\n",
                "This notebook demonstrates the core research workflow of the Deep Research Agent, including:\n",
                "1. Query Decomposition\n",
                "2. Web Search & RAG Ingestion\n",
                "3. Evidence Auditing & Verification\n",
                "4. Final Synthesis\n",
                "5. Artifact Export"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup & Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%load_ext autoreload\n",
                "%autoreload 2\n",
                "\n",
                "import sys\n",
                "import os\n",
                "\n",
                "# Add backend/src to path so we can import modules\n",
                "sys.path.append(os.path.abspath(\"../backend/src\"))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from agent.deep_search_agent import DeepSearchAgent\n",
                "from langchain_google_genai import ChatGoogleGenerativeAI\n",
                "from agent.configuration import Configuration\n",
                "import logging\n",
                "\n",
                "# Configure logging\n",
                "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Initialize Agent Components"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize LLM\n",
                "# We use the SELECTED_MODEL from the config cell above\n",
                "model_name = os.environ.get(\"ANSWER_MODEL\", \"gemini-1.5-flash\")\n",
                "\n",
                "try:\n",
                "    print(f\"Initializing LLM with model: {model_name}\")\n",
                "    llm = ChatGoogleGenerativeAI(model=model_name, temperature=0)\n",
                "except Exception as e:\n",
                "    print(f\"Error initializing LLM: {e}\")\n",
                "    # Fallback for testing\n",
                "    llm = None\n",
                "\n",
                "if llm:\n",
                "    # Initialize Agent\n",
                "    # Note: DeepSearchAgent internally uses the Configuration object which reads from env vars\n",
                "    agent = DeepSearchAgent(llm_client=llm)\n",
                "    print(\"Agent initialized successfully.\")\n",
                "else:\n",
                "    print(\"Failed to initialize Agent due to LLM error.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Single Query Research Demo"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "query = \"Latest advances in renewable energy storage\"\n",
                "print(f\"Starting research on: {query}\")\n",
                "\n",
                "if agent:\n",
                "    final_answer = agent.research(query)\n",
                "    print(\"\\n=== FINAL ANSWER ===\\n\")\n",
                "    print(final_answer)\n",
                "else:\n",
                "    print(\"Agent not initialized.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. RAG Verification Deep Dive\n",
                "\n",
                "Inspect the internal state of the RAG system to verify ingestion and retrieval."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if agent:\n",
                "    # Inspect Doc Store\n",
                "    print(f\"Total documents indexed: {len(agent.rag.doc_store)}\")\n",
                "\n",
                "    # Sample a document\n",
                "    if len(agent.rag.doc_store) > 0:\n",
                "        doc_id = list(agent.rag.doc_store.keys())[0]\n",
                "        print(f\"\\nSample Document ({doc_id}):\")\n",
                "        print(agent.rag.doc_store[doc_id])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if agent:\n",
                "    # Test Retrieval\n",
                "    test_query = \"battery cost reduction\"\n",
                "    results = agent.rag.retrieve(test_query, top_k=3)\n",
                "\n",
                "    print(f\"\\nRetrieval results for '{test_query}':\")\n",
                "    for doc, score in results:\n",
                "        print(f\"- [{score:.2f}] {doc.content[:100]}...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Export Research Artifacts"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if agent:\n",
                "    state = agent.rag.export_state()\n",
                "    print(\"RAG State Export:\", state)\n",
                "\n",
                "    # Retrieve all docs for export\n",
                "    all_docs = agent.get_retrieved_documents()\n",
                "    print(f\"\\nPrepared {len(all_docs)} documents for export.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}