{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üèõÔ∏è Agent Architecture Demo\n",
                "\n",
                "This notebook visualizes the internal structure of the LangGraph agent and demonstrates its execution flow.\n",
                "\n",
                "## Features\n",
                "- **Graph Visualization**: See the nodes and edges of the agent\n",
                "- **Step-by-Step Execution**: Trace how the agent processes a query\n",
                "- **State Inspection**: View the agent's internal state at each step\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === COLAB SETUP (Auto-detects environment) ===\n",
                "try:\n",
                "    import google.colab\n",
                "    IN_COLAB = True\n",
                "except ImportError:\n",
                "    IN_COLAB = False\n",
                "\n",
                "if IN_COLAB:\n",
                "    print(\"üîß Running in Google Colab - Setting up environment...\")\n",
                "    \n",
                "    # Clone repository\n",
                "    !git clone https://github.com/MasumRab/gemini-fullstack-langgraph-quickstart --depth 1\n",
                "    %cd gemini-fullstack-langgraph-quickstart/backend\n",
                "    \n",
                "    # Clean conflicting packages\n",
                "    !pip uninstall -y google-ai-generativelanguage google-generativeai tensorflow grpcio-status 2>/dev/null || true\n",
                "    \n",
                "    # Install backend dependencies\n",
                "    !pip install -q .\n",
                "    \n",
                "    # Set API Key from Colab secrets\n",
                "    import os\n",
                "    from google.colab import userdata\n",
                "    try:\n",
                "        os.environ[\"GEMINI_API_KEY\"] = userdata.get('GEMINI_API_KEY')\n",
                "        print(\"‚úÖ API Key loaded from Colab Secrets\")\n",
                "    except:\n",
                "        print(\"‚ö†Ô∏è API Key not found in secrets. Please set it manually:\")\n",
                "        os.environ[\"GEMINI_API_KEY\"] = input(\"Enter your Gemini API Key: \")\n",
                "else:\n",
                "    print(\"üíª Running locally - Assuming dependencies are installed\")\n",
                "    import os\n",
                "    import sys\n",
                "    # Add backend/src to path for local development\n",
                "    sys.path.insert(0, os.path.abspath(\"../backend/src\"))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- MODEL CONFIGURATION ---\n",
                "# @title Select Gemini Model\n",
                "# @markdown Choose the model strategy to use. Use 'Free Tier' if you encounter Rate Limits.\n",
                "\n",
                "MODEL_STRATEGY = \"Free Tier (Gemini 1.5 Flash)\" # @param [\"Free Tier (Gemini 1.5 Flash)\", \"Experimental (Gemini 2.0 Flash)\", \"Flash Lite (Gemini 2.0 Flash-Lite)\"]\n",
                "\n",
                "import os\n",
                "\n",
                "# Map selection to actual model names\n",
                "if MODEL_STRATEGY == \"Free Tier (Gemini 1.5 Flash)\":\n",
                "    SELECTED_MODEL = \"gemini-1.5-flash\"\n",
                "    print(\"‚úÖ Using Free Tier Model: gemini-1.5-flash\")\n",
                "elif MODEL_STRATEGY == \"Experimental (Gemini 2.0 Flash)\":\n",
                "    SELECTED_MODEL = \"gemini-2.0-flash-exp\"\n",
                "    print(\"‚ö†Ô∏è Using Experimental Model: gemini-2.0-flash-exp (Quota Required)\")\n",
                "elif MODEL_STRATEGY == \"Flash Lite (Gemini 2.0 Flash-Lite)\":\n",
                "    SELECTED_MODEL = \"gemini-2.0-flash-lite-preview-02-05\"\n",
                "    print(\"‚ö†Ô∏è Using Preview Model: gemini-2.0-flash-lite-preview-02-05 (Quota Required)\")\n",
                "\n",
                "# Set Environment Variables to override defaults\n",
                "os.environ[\"QUERY_GENERATOR_MODEL\"] = SELECTED_MODEL\n",
                "os.environ[\"REFLECTION_MODEL\"] = SELECTED_MODEL\n",
                "os.environ[\"ANSWER_MODEL\"] = SELECTED_MODEL\n",
                "os.environ[\"TOOLS_MODEL\"] = SELECTED_MODEL"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load the Agent Graph\n",
                "\n",
                "We import the compiled graph object from the backend code."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import the graph\n",
                "try:\n",
                "    from agent.graph import graph\n",
                "    print(\"‚úÖ Graph imported successfully\")\n",
                "except ImportError as e:\n",
                "    print(f\"‚ùå Failed to import graph: {e}\")\n",
                "    print(\"Ensure you are in the correct directory (backend/) and dependencies are installed.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Visualize the Architecture\n",
                "\n",
                "We use Mermaid.js to render the graph structure directly in the notebook."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from IPython.display import Image, display\n",
                "\n",
                "print(\"üìä Generating Graph Visualization...\")\n",
                "\n",
                "try:\n",
                "    # Draw the graph as a PNG\n",
                "    png_data = graph.get_graph().draw_mermaid_png()\n",
                "    display(Image(png_data))\n",
                "    print(\"‚úÖ Visualization complete\")\n",
                "except Exception as e:\n",
                "    print(f\"‚ö†Ô∏è Graphical visualization failed: {e}\")\n",
                "    print(\"\\nFalling back to ASCII representation:\")\n",
                "    print(\"=\"*40)\n",
                "    graph.get_graph().print_ascii()\n",
                "    print(\"=\"*40)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Run a Demo Query\n",
                "\n",
                "Execute a simple query to see the agent in action."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain_core.messages import HumanMessage\n",
                "import asyncio\n",
                "\n",
                "async def run_demo(question: str):\n",
                "    \"\"\"Run the agent and stream output.\"\"\"\n",
                "    print(f\"ü§ñ Processing Query: \\\"{question}\\\"\")\n",
                "    print(\"-\" * 50)\n",
                "\n",
                "    config = {\"configurable\": {\"thread_id\": \"demo_notebook\"}}\n",
                "    \n",
                "    async for event in graph.astream_events(\n",
                "        {\"messages\": [HumanMessage(content=question)]},\n",
                "        config,\n",
                "        version=\"v2\"\n",
                "    ):\n",
                "        kind = event[\"event\"]\n",
                "        \n",
                "        if kind == \"on_chat_model_stream\":\n",
                "            content = event[\"data\"][\"chunk\"].content\n",
                "            if content:\n",
                "                print(content, end=\"\", flush=True)\n",
                "                \n",
                "        elif kind == \"on_tool_start\":\n",
                "            print(f\"\\n\\nüõ†Ô∏è Executing Tool: {event['name']}\")\n",
                "            \n",
                "        elif kind == \"on_tool_end\":\n",
                "            print(f\"‚úÖ Tool Completed: {event['name']}\\n\")\n",
                "\n",
                "    print(\"\\n\" + \"-\" * 50)\n",
                "    print(\"üèÅ Execution Complete\")\n",
                "\n",
                "# Run the demo\n",
                "QUERY = \"What is the capital of France?\"\n",
                "await run_demo(QUERY)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## üìö Learn More\n",
                "\n",
                "- **Visualization Guide**: `docs/guides/VISUALIZATION_EXAMPLES.md`\n",
                "- **LangGraph Docs**: [https://langchain-ai.github.io/langgraph/](https://langchain-ai.github.io/langgraph/)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}