{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- COLAB SETUP START ---\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    # 1. Clone the repository\n",
<<<<<<< HEAD
    "    !git clone https://github.com/google-gemini/gemini-fullstack-langgraph-quickstart\n",
=======
    "    !git clone https://github.com/MasumRab/gemini-fullstack-langgraph-quickstart\n",
>>>>>>> origin/main
    "    %cd gemini-fullstack-langgraph-quickstart/backend\n",
    "\n",
    "    # 2. Prepare Environment (Resolving Conflicts)\n",
    "    import sys\n",
    "    print(\"Uninstalling conflicting pre-installed packages...\")\n",
<<<<<<< HEAD
    "    !pip uninstall -y google-ai-generativelanguage tensorflow grpcio-status\n",
=======
    "    !pip uninstall -y google-ai-generativelanguage google-generativeai tensorflow grpcio-status\n",
>>>>>>> origin/main
    "\n",
    "    # Pre-install PyTorch Nightly if Python 3.13+ is detected\n",
    "    if sys.version_info >= (3, 13):\n",
    "        print(\"Detected Python 3.13+. Installing PyTorch Nightly...\")\n",
    "        !pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cpu\n",
    "\n",
    "    # 3. Install Backend\n",
    "    !pip install .\n",
    "\n",
    "    # 4. Set API Key\n",
    "    import os\n",
    "    from google.colab import userdata\n",
    "    try:\n",
    "        os.environ[\"GEMINI_API_KEY\"] = userdata.get('GEMINI_API_KEY')\n",
    "    except:\n",
    "        print(\"Please enter your Gemini API Key:\")\n",
    "        os.environ[\"GEMINI_API_KEY\"] = input()\n",
    "# --- COLAB SETUP END ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemini Fullstack LangGraph Quickstart - Agent Architecture Demo\n",
    "\n",
    "This notebook demonstrates the core agent architecture used in the backend. It visualizes the graph and runs a simple query through the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup environment\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Add backend/src to path\n",
    "sys.path.append(os.path.abspath(\"../backend/src\"))\n",
    "\n",
    "load_dotenv(\"../backend/.env\")\n",
    "\n",
    "# Verify API keys are present (optional check)\n",
<<<<<<< HEAD
    "if not os.getenv(\"GOOGLE_API_KEY\"):\n",
    "    print(\"Warning: GOOGLE_API_KEY not found in environment\")"
=======
    "if not os.getenv(\"GEMINI_API_KEY\"):\n",
    "    print(\"Warning: GEMINI_API_KEY not found in environment\")"
>>>>>>> origin/main
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Visualize the Agent Graph\n",
    "\n",
    "We'll load the main agent graph and visualize its structure using Mermaid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent.graph import graph\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    # Generate Mermaid PNG\n",
    "    png_data = graph.get_graph().draw_mermaid_png()\n",
    "    display(Image(png_data))\n",
    "except Exception as e:\n",
    "    print(f\"Could not draw graph: {e}\")\n",
    "    # Fallback to ascii\n",
    "    graph.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run a Simple Query\n",
    "\n",
    "Let's run a basic query through the agent to see the step-by-step execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "async def run_demo():\n",
    "    input_message = HumanMessage(content=\"What is the capital of France?\")\n",
    "    config = {\"configurable\": {\"thread_id\": \"demo_notebook_1\"}}\n",
    "    \n",
    "    print(f\"User: {input_message.content}\\n\")\n",
    "    \n",
    "    async for event in graph.astream_events(\n",
    "        {\"messages\": [input_message]},\n",
    "        config,\n",
    "        version=\"v2\"\n",
    "    ):\n",
    "        kind = event[\"event\"]\n",
    "        if kind == \"on_chat_model_stream\":\n",
    "            content = event[\"data\"][\"chunk\"].content\n",
    "            if content:\n",
    "                print(content, end=\"\", flush=True)\n",
    "        elif kind == \"on_tool_start\":\n",
    "            print(f\"\\n[Tool Start: {event['name']}]\", flush=True)\n",
    "        elif kind == \"on_tool_end\":\n",
    "            print(f\"\\n[Tool End: {event['name']}]\", flush=True)\n",
    "\n",
    "# Run the async function\n",
    "# Note: In Jupyter, you can usually await directly. If running as script, use asyncio.run()\n",
    "await run_demo()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}