{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Gemini Fullstack LangGraph Quickstart - Agent Architecture Demo\n",
                "\n",
                "This notebook demonstrates the core agent architecture used in the backend. It visualizes the graph and runs a simple query through the agent."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Setup environment\n",
                "import os\n",
                "import sys\n",
                "from dotenv import load_dotenv\n",
                "\n",
                "# Add backend/src to path\n",
                "sys.path.append(os.path.abspath(\"../backend/src\"))\n",
                "\n",
                "load_dotenv(\"../backend/.env\")\n",
                "\n",
                "# Verify API keys are present (optional check)\n",
                "if not os.getenv(\"GOOGLE_API_KEY\"):\n",
                "    print(\"Warning: GOOGLE_API_KEY not found in environment\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Visualize the Agent Graph\n",
                "\n",
                "We'll load the main agent graph and visualize its structure using Mermaid."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from agent.graph import graph\n",
                "from IPython.display import Image, display\n",
                "\n",
                "try:\n",
                "    # Generate Mermaid PNG\n",
                "    png_data = graph.get_graph().draw_mermaid_png()\n",
                "    display(Image(png_data))\n",
                "except Exception as e:\n",
                "    print(f\"Could not draw graph: {e}\")\n",
                "    # Fallback to ascii\n",
                "    graph.get_graph().print_ascii()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Run a Simple Query\n",
                "\n",
                "Let's run a basic query through the agent to see the step-by-step execution."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import asyncio\n",
                "from langchain_core.messages import HumanMessage\n",
                "\n",
                "async def run_demo():\n",
                "    input_message = HumanMessage(content=\"What is the capital of France?\")\n",
                "    config = {\"configurable\": {\"thread_id\": \"demo_notebook_1\"}}\n",
                "    \n",
                "    print(f\"User: {input_message.content}\\n\")\n",
                "    \n",
                "    async for event in graph.astream_events(\n",
                "        {\"messages\": [input_message]},\n",
                "        config,\n",
                "        version=\"v2\"\n",
                "    ):\n",
                "        kind = event[\"event\"]\n",
                "        if kind == \"on_chat_model_stream\":\n",
                "            content = event[\"data\"][\"chunk\"].content\n",
                "            if content:\n",
                "                print(content, end=\"\", flush=True)\n",
                "        elif kind == \"on_tool_start\":\n",
                "            print(f\"\\n[Tool Start: {event['name']}]\", flush=True)\n",
                "        elif kind == \"on_tool_end\":\n",
                "            print(f\"\\n[Tool End: {event['name']}]\", flush=True)\n",
                "\n",
                "# Run the async function\n",
                "# Note: In Jupyter, you can usually await directly. If running as script, use asyncio.run()\n",
                "await run_demo()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}