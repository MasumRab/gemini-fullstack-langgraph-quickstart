{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepResearch-Bench Evaluation Pipeline\n",
    "\n",
    "This notebook implements the complete evaluation pipeline for DeepResearch-Bench, including:\n",
    "1. Data Preparation (Download/Mock)\n",
    "2. Metric Verification\n",
    "3. Full Benchmark Execution\n",
    "4. Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Add backend/src to path\n",
    "sys.path.append(os.path.abspath(\"../backend/src\"))\n",
    "\n",
    "from evaluation.data import download_benchmark_data\n",
    "from evaluation.bench import BenchmarkEvaluator\n",
    "from agent.deep_search_agent import DeepSearchAgent\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data/benchmark\"\n",
    "print(f\"Setting up benchmark data in {data_dir}...\")\n",
    "download_benchmark_data(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup Agent & Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Agent\n",
    "try:\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", temperature=0)\n",
    "except Exception:\n",
    "    print(\"Using Mock LLM for benchmark setup\")\n",
    "    class MockLLM:\n",
    "        def invoke(self, prompt):\n",
    "            # Return generic JSON for verifications\n",
    "            if \"JSON\" in str(prompt):\n",
    "                if \"claims\" in str(prompt):\n",
    "                    return '```json\\n{\"claims\": [\"Claim 1\", \"Claim 2\"]}\\n```'\n",
    "                return '```json\\n{\"verified\": true, \"confidence\": 0.9, \"reasoning\": \"good\"}\\n```'\n",
    "            return \"Evaluated Answer\"\n",
    "        def generate(self, prompt): return self.invoke(prompt)\n",
    "    llm = MockLLM()\n",
    "\n",
    "agent = DeepSearchAgent(llm_client=llm)\n",
    "\n",
    "# Initialize Evaluator\n",
    "evaluator = BenchmarkEvaluator(agent, data_dir=data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Benchmark\n",
    "\n",
    "We will run the benchmark on the available data (mock or real)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting Evaluation...\")\n",
    "results = evaluator.evaluate_full_benchmark(\n",
    "    output_file=\"../results/benchmark_run.json\"\n",
    ")\n",
    "\n",
    "print(\"Evaluation Finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results:\n",
    "    print(\"Final Scores:\")\n",
    "    print(json.dumps(results, indent=2))\n",
    "else:\n",
    "    print(\"No results to analyze.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
