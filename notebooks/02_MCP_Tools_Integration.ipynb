{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431d840e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Universal Setup for Backend Environment\n",
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "def setup_environment():\n",
    "    \"\"\"Setup the environment by installing necessary dependencies and setting paths.\"\"\"\n",
    "    # Get the backend directory. If we are in 'backend', it is cwd.\n",
    "    backend_dir = Path.cwd()\n",
    "    if backend_dir.name != 'backend':\n",
    "        # Search for backend\n",
    "        if (backend_dir / 'backend').exists():\n",
    "             backend_dir = backend_dir / 'backend'\n",
    "        elif (backend_dir.parent / 'backend').exists():\n",
    "             backend_dir = backend_dir.parent / 'backend'\n",
    "    \n",
    "    # Add src to path if it exists (for 'from agent import ...' style)\n",
    "    src_dir = backend_dir / 'src'\n",
    "    if src_dir.exists():\n",
    "        if str(src_dir) not in sys.path:\n",
    "            sys.path.append(str(src_dir))\n",
    "            print(f\"  [OK] Added {src_dir} to sys.path\")\n",
    "    \n",
    "    if str(backend_dir) not in sys.path:\n",
    "        sys.path.append(str(backend_dir))\n",
    "        \n",
    "    # Verify backend/agent can be imported\n",
    "    try:\n",
    "        import agent\n",
    "        print(\"  [OK] Agent module found and imported.\")\n",
    "    except ImportError:\n",
    "        print(\"  [!] Agent module not found. Installing dependencies...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-e\", str(backend_dir)])\n",
    "        print(\"  [OK] Backend installed in editable mode.\")\n",
    "\n",
    "setup_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536f3174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- COLAB SETUP START ---\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"üîß Running in Google Colab - Installing dependencies...\")\n",
    "    \n",
    "    # 1. Clone the repository\n",
    "    !rm -rf gemini-fullstack-langgraph-quickstart\n",
    "    !git clone https://github.com/MasumRab/gemini-fullstack-langgraph-quickstart.git\n",
    "    \n",
    "    # 2. Navigate to the correct directory\n",
    "    import os\n",
    "    repo_name = \"gemini-fullstack-langgraph-quickstart\"\n",
    "    target_dir = os.path.join(repo_name, \"notebooks\")\n",
    "    \n",
    "    if os.path.exists(target_dir):\n",
    "        os.chdir(target_dir)\n",
    "        print(f\"  [OK] Changed directory to {os.getcwd()}\")\n",
    "    else:\n",
    "        # Fallback to repo root if specific dir not found\n",
    "        if os.path.exists(repo_name):\n",
    "            os.chdir(repo_name)\n",
    "            print(f\"  [OK] Changed directory to {os.getcwd()} (Fallback)\")\n",
    "    \n",
    "    # 3. Install Backend (Quietly)\n",
    "    # We install from the backend directory which should be reachable\n",
    "    # relative to current dir or absolute\n",
    "    \n",
    "    # Find backend relative to current position\n",
    "    import sys\n",
    "    if os.path.exists(\"backend\"):\n",
    "        !pip install -q -e backend\n",
    "    elif os.path.exists(\"../backend\"):\n",
    "        !pip install -q -e ../backend\n",
    "    elif os.path.exists(\"src\"): # We might be IN backend\n",
    "        !pip install -q -e .\n",
    "        \n",
    "    print(\"  [OK] Dependencies installed!\")\n",
    "else:\n",
    "    print(\"  [OK] Running locally\")\n",
    "# --- COLAB SETUP END ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92790d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MODEL CONFIGURATION ---\n",
    "# @title Select Gemini Model\n",
    "# @markdown Choose the Gemini model to use. Only Gemini 2.5 models are currently accessible via the API.\n",
    "\n",
    "MODEL_STRATEGY = \"Gemini 2.5 Flash (Recommended)\" # @param [\"Gemini 2.5 Flash (Recommended)\", \"Gemini 2.5 Flash-Lite (Fastest)\", \"Gemini 2.5 Pro (Best Quality)\"]\n",
    "\n",
    "import os\n",
    "\n",
    "# Map selection to model ID\n",
    "# Note: Gemini 1.5 and 2.0 models are deprecated/not accessible via this API\n",
    "if MODEL_STRATEGY == \"Gemini 2.5 Flash (Recommended)\":\n",
    "    SELECTED_MODEL = \"gemma-3-27b-it\"\n",
    "elif MODEL_STRATEGY == \"Gemini 2.5 Flash-Lite (Fastest)\":\n",
    "    SELECTED_MODEL = \"gemma-3-27b-it-lite\"\n",
    "elif MODEL_STRATEGY == \"Gemini 2.5 Pro (Best Quality)\":\n",
    "    SELECTED_MODEL = \"gemma-3-27b-it\"\n",
    "else:\n",
    "    # Default fallback\n",
    "    SELECTED_MODEL = \"gemma-3-27b-it\"\n",
    "\n",
    "print(f\"Selected Model: {SELECTED_MODEL}\")\n",
    "print(f\"Strategy: {MODEL_STRATEGY}\")\n",
    "\n",
    "# Set Environment Variables to override defaults\n",
    "os.environ[\"QUERY_GENERATOR_MODEL\"] = SELECTED_MODEL\n",
    "os.environ[\"REFLECTION_MODEL\"] = SELECTED_MODEL\n",
    "os.environ[\"ANSWER_MODEL\"] = SELECTED_MODEL\n",
    "os.environ[\"TOOLS_MODEL\"] = SELECTED_MODEL\n",
    "\n",
    "# Ensure GOOGLE_API_KEY is set if GEMINI_API_KEY is present (for LangChain compatibility)\n",
    "if \"GEMINI_API_KEY\" in os.environ:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = os.environ[\"GEMINI_API_KEY\"]\n",
    "    print(\"  [OK] Synced GEMINI_API_KEY to GOOGLE_API_KEY for LangChain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178c7f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MODEL VERIFICATION (Optional) ---\n",
    "# @title Verify Model Configuration\n",
    "# @markdown Run this cell to verify that your API key is configured correctly and the selected model is available.\n",
    "\n",
    "import os\n",
    "\n",
    "# Check if API key is set\n",
    "if \"GEMINI_API_KEY\" not in os.environ:\n",
    "    print(\"‚ö†Ô∏è  GEMINI_API_KEY not found in environment variables!\")\n",
    "    print(\"   Please set it before proceeding:\")\n",
    "    print(\"   export GEMINI_API_KEY='your-api-key-here'\")\n",
    "else:\n",
    "    try:\n",
    "        from google import genai\n",
    "        \n",
    "        # Initialize the client\n",
    "        client = genai.Client(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "        \n",
    "        # Test the selected model\n",
    "        print(f\"üß™ Testing model: {SELECTED_MODEL}\")\n",
    "        response = client.models.generate_content(\n",
    "            model=SELECTED_MODEL,\n",
    "            contents=\"Explain how AI works in a few words\"\n",
    "        )\n",
    "        \n",
    "        print(f\"  [OK] Model verification successful!\")\n",
    "        print(f\"   Model: {SELECTED_MODEL}\")\n",
    "        print(f\"   Response: {response.text[:100]}...\")\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"  [!] google-genai package not installed!\")\n",
    "        print(\"   Installing now...\")\n",
    "        import subprocess\n",
    "        import sys\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"google-genai\"])\n",
    "        print(\"  [OK] Installed! Please re-run this cell.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  [X] Model verification failed: {e}\")\n",
    "        print(f\"   This could mean:\")\n",
    "        print(f\"   - Invalid API key\")\n",
    "        print(f\"   - Model '{SELECTED_MODEL}' not available in your region\")\n",
    "        print(f\"   - Quota/billing issues (for experimental models)\")\n",
    "        print(f\"   - Network connectivity issues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148d64e2",
   "metadata": {},
   "source": [
    "# MCP Tools: Planned Tool Use\n",
    "\n",
    "This notebook demonstrates the Model Context Protocol (MCP) integration, specifically:\n",
    "1. Setting up a Filesystem MCP Server\n",
    "2. Planning tool sequences with an LLM\n",
    "3. Executing file operations safely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f1aff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Add backend/src to path\n",
    "sys.path.append(os.path.abspath(\"../backend/src\"))\n",
    "\n",
    "from agent.mcp_server import FilesystemMCPServer\n",
    "from agent.mcp_client import MCPToolUser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e514c9e6",
   "metadata": {},
   "source": [
    "## 1. Setup MCP Server\n",
    "\n",
    "We create a sandbox directory and initialize the server allowing access only to that directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d416da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup sandbox\n",
    "sandbox_dir = \"./mcp_sandbox\"\n",
    "os.makedirs(sandbox_dir, exist_ok=True)\n",
    "\n",
    "# Initialize Server\n",
    "fs_server = FilesystemMCPServer(allowed_paths=[sandbox_dir])\n",
    "print(f\"Filesystem Server initialized. Root: {sandbox_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f713a4",
   "metadata": {},
   "source": [
    "## 2. Initialize Tool User (Client)\n",
    "\n",
    "The client aggregates tools from servers and handles planning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e408af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcp_client = MCPToolUser(mcp_servers=[fs_server])\n",
    "\n",
    "# List available tools\n",
    "print(\"Available Tools:\")\n",
    "for name in mcp_client.tool_registry:\n",
    "    print(f\"- {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ee4272",
   "metadata": {},
   "source": [
    "## 3. Basic Tool Operations (Manual Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af972fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "async def run_manual_test():\n",
    "    # Write\n",
    "    print(\"Writing file...\")\n",
    "    res = await mcp_client.execute_tool(\n",
    "        \"filesystem.write_file\", \n",
    "        path=f\"{sandbox_dir}/test.txt\", \n",
    "        content=\"Hello MCP World!\"\n",
    "    )\n",
    "    print(\"Write Result:\", res)\n",
    "\n",
    "    # Read\n",
    "    print(\"\\nReading file...\")\n",
    "    res = await mcp_client.execute_tool(\n",
    "        \"filesystem.read_file\", \n",
    "        path=f\"{sandbox_dir}/test.txt\"\n",
    "    )\n",
    "    print(\"Read Result:\", res)\n",
    "\n",
    "await run_manual_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83052d6e",
   "metadata": {},
   "source": [
    "## 4. Tool Planning Demo\n",
    "\n",
    "We ask the LLM to plan a sequence of operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb800e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM\n",
    "try:\n",
    "    model_name = os.environ.get(\"ANSWER_MODEL\", \"gemma-3-27b-it\")\n",
    "    print(f\"Initializing LLM with model: {model_name}\")\n",
    "    llm = ChatGoogleGenerativeAI(model=model_name, temperature=0)\n",
    "except Exception as e:\n",
    "    print(\"Using Mock LLM\")\n",
    "    class MockLLM:\n",
    "        def invoke(self, prompt): return '```json\\n[{\"tool\": \"filesystem.write_file\", \"params\": {\"path\": \"./mcp_sandbox/plan.txt\", \"content\": \"Step 1: Done\"}}]\\n```'\n",
    "    llm = MockLLM()\n",
    "\n",
    "# Scenario\n",
    "task = f\"Create a new directory named 'reports' inside '{sandbox_dir}' and write a file named 'summary.md' inside it with the text 'Analysis Complete'.\"\n",
    "\n",
    "# 1. Plan\n",
    "plan = mcp_client.plan_tool_sequence(task, llm)\n",
    "print(\"Generated Plan:\")\n",
    "print(json.dumps(plan, indent=2))\n",
    "\n",
    "# 2. Execute\n",
    "print(\"\\nExecuting Plan...\")\n",
    "import json\n",
    "if plan:\n",
    "    results = await mcp_client.execute_plan(plan)\n",
    "    for r in results:\n",
    "        print(f\"Tool: {r['tool']}, Success: {r['result'].get('success')}\")\n",
    "else:\n",
    "    print(\"Plan generation failed or empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc413a5",
   "metadata": {},
   "source": [
    "## 5. Security & Constraints\n",
    "\n",
    "Verify that we cannot write outside the sandbox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31102d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_security():\n",
    "    print(\"Attempting to write to /tmp (outside sandbox)...\")\n",
    "    res = await mcp_client.execute_tool(\n",
    "        \"filesystem.write_file\", \n",
    "        path=\"/tmp/hacked.txt\", \n",
    "        content=\"Should fail\"\n",
    "    )\n",
    "    print(\"Result:\", res)\n",
    "\n",
    "await test_security()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mcp-implementation-todos",
   "metadata": {},
   "source": [
    "## 6. Pending MCP Features (TODO)\n",
    "\n",
    "The following demos will be added once the backend features are implemented:\n",
    "\n",
    "### TODO: State Persistence (Open SWE)\n",
    "- [ ] Demonstrate `save_plan_tool` writing the current agent state to JSON.\n",
    "- [ ] Demonstrate `load_plan_tool` recovering the state after a restart.\n",
    "\n",
    "### TODO: Collaborative Artifacts (Open Canvas)\n",
    "- [ ] Demonstrate `update_artifact` creating a live Markdown document.\n",
    "- [ ] Show how multiple agent steps refine the same artifact file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
